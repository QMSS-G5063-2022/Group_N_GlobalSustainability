library(pdftools)
library(tidyverse)
library(ggplot2)
library(tm)
#install.packages("tesseract")
library(tesseract)
#install.packages("officer")
library(officer)
#install.packages("textreadr")
library(textreadr)
doc <- htmlParse("https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52019DC0640&from=EN")
library(XML)
doc <- htmlParse("https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52019DC0640&from=EN")
url = "https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52019DC0640&from=EN"
mydata = read.HTMLTable(url, which = 11, trim = T)
readtext(paste0(DATA_DIR, "/Group_N_GlobalSustainability/*"))
library(readtext)
install.packages("readtext")
library(readtext)
readtext(paste0(DATA_DIR, "/Group_N_GlobalSustainability/*"))
readtext(paste0("/Group_N_GlobalSustainability/*"))
# Read a text file using read.delim2
myData = read.delim2("eu_green_deal.txt", header = FALSE)
print(myData)
View(myData)
View(myData)
# Read a text file using read.delim2
green_deal = read.delim2("eu_green_deal.txt", header = FALSE)
print(green_deal)
head(green_deal)
print(green_deal)
rm(myData)
green_deal <- green_deal %>%
rename(text = V1)
print(green_deal)
library(quanteda) # Another great text mining package
library(tidytext)
data("stop_words")
library(tidyverse)
library(ggplot2)
library(tm)
library(quanteda)
library(tidytext)
data("stop_words")
library(textreadr)
green_deal <- green_deal %>%
dplyr::mutate(doc_id = row_number()) %>%
relocate(doc_id)
View(green_deal)
class(green_deal)
# Convert df_source to a volatile corpus
green_deal_corpus <- VCorpus(green_deal)
# Convert df_source to a volatile corpus
green_deal_df_source <- DataframeSource(green_deal)
View(green_deal_df_source)
View(green_deal_df_source)
green_deal_corpus <- VCorpus(green_deal_df_source)
View(green_deal_corpus)
#remove punctuation
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(removePunctuation))
#remove stop words
green_deal_corpus <- tm_map(green_deal_corpus, removeWords, stopwords("english"))
#remove stop words
green_deal_corpus <- green_deal_corpus %>%
tm_map(green_deal_corpus, removeWords, stopwords("english")) %>%
tm_map(green_deal_corpus, content_transformer(removePunctuation))
#remove punctuation
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(removePunctuation))
#remove numbers
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(removeNumbers))
#to lowercase
green_deal_corpus <- tm_map(green_deal_corpus,  content_transformer(tolower))
#remove whitespace
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(stripWhitespace))
#remove special characters and htmp (if present)
removeNonASCII <- function(x) iconv(x, "latin1", "ASCII", sub="")
removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(removeURL)) #remove web url
green_deal_corpus <- tm_map(green_deal_corpus, content_transformer(removeNonASCII)) #remove non-ASCII
# Print data on the 15th document -> originally, there were come non ASCII letters but now it is all clean
green_deal_corpus[[15]]$content
#document term matrix version2
green_deal_DTM <- DocumentTermMatrix(green_deal_corpus)
inspect(green_deal_DTM)
# Convert sotu_tdm to a matrix: sotu_m
green_deal_matrix <- as.matrix(green_deal_DTM)
library(textstem)
library(tidyverse)
library(ggplot2)
library(tm)
library(quanteda)
library(tidytext)
data("stop_words")
library(textreadr)
library(textstem) #word clounds
#tidy the corpus in a tidy dataframe
green_deal_td <- tidy(green_deal_corpus)
View(green_deal_td)
green_deal_words <- green_deal_td %>%
unnest_tokens(word, text) %>% #I noticed that there are still some stop words left
anti_join(stop_words) %>%
mutate(lemma = lemmatize_words(word)) %>% #lemmatize
count(lemma, sort = TRUE) %>%
dplyr::rename(n_success = n)
head(success_words)
head(green_deal_words)
View(green_deal_td)
View(green_deal_words)
#head(green_deal_words)
library(wordcloud)
set.seed(1234)
wordcloud(words = green_deal_words$lemma, freq = green_deal_words$n_success, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = green_deal_words$lemma, freq = green_deal_words$n_success, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
ggplot(aes(lemma, fill = n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
ggplot(data = green_deal_words, aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
mutate(lemma = reorder(lemma, value)) %>%
ggplot(aes(lemma, fill = n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, fill = n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
arrange(desc(n_success)) %>%
dplyr::slice(1:10) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
arrange(desc(n_success)) %>%
dplyr::slice(1:10) %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
scale_fill_discrete(name = "Project Status", labels = c("Failed", "Succeeded")) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
arrange(desc(n_success)) %>%
dplyr::slice(1:10) %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in Kickstarter Projects")
green_deal_words %>%
arrange(desc(n_success)) %>%
dplyr::slice(1:10) %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in the EU Green Deal")
green_deal_words %>%
arrange(desc(n_success)) %>%
dplyr::slice(1:10) %>%
mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in the EU Green Deal") +
coord_flip()
library(tidyverse)
library(ggplot2)
library(tm)
library(quanteda)
library(tidytext)
data("stop_words")
library(textreadr)
library(textstem) #word clounds
# Read a text file using read.delim2
green_deal = read.delim2("eu_green_deal.txt", header = FALSE)
green_deal <- green_deal %>%
rename(text = V1)
print(green_deal)
library(tidyverse)
library(ggplot2)
library(tm)
library(quanteda)
library(tidytext)
data("stop_words")
library(textreadr)
library(textstem) #word clounds
library(pdftools)
files <- list.files(pattern = "pdf$")
opinions <- lapply(files, pdf_text)
corp <- VCorpus(URISource(files),
readerControl = list(reader = readPDF))
#remove stop words
ipcc_corpus <- tm_map(corp, removeWords, stopwords("english"))
#remove punctuation
ipcc_corpus <- tm_map(corp, content_transformer(removePunctuation))
#remove numbers
#ipcc_corpus <- tm_map(corp, content_transformer(removeNumbers))
#to lowercase
ipcc_corpus <- tm_map(corp,  content_transformer(tolower))
#remove whitespace
ipcc_corpus <- tm_map(corp, content_transformer(stripWhitespace))
#remove special characters and htmp (if present)
removeNonASCII <- function(x) iconv(x, "latin1", "ASCII", sub="")
#removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
#ipcc_corpus <- tm_map(corp, content_transformer(removeURL)) #remove web url
ipcc_corpus <- tm_map(corp, content_transformer(removeNonASCII)) #remove non-ASCII
#lemmatize
ipcc_corpus  <- tm_map(corp, content_transformer(textstem::lemmatize_strings))
# Print data on the 15th document -> originally, there were come non ASCII letters but now it is all clean
#ipcc_corpus[[2]]$content
ipcc_tdm <- TermDocumentMatrix(ipcc_corpus,
control = list(bounds = list(global = c(5, Inf))))
#list(removePunctuation = TRUE,
#    stopwords = TRUE,
#   tolower = TRUE,
#stemming = TRUE,
#  removeNumbers = TRUE,
#bounds = list(global = c(5, Inf)))
#filter for words that have at least 100 frequency
ipcc_ft <- findFreqTerms(ipcc_tdm, lowfreq = 100, highfreq = Inf)
ipcc_ftm <- as.matrix(ipcc_tdm[ipcc_ft,], id = nDocs(ipcc_tdm))
#v <- sort(apply(ipcc_ftm, 1, sum), decreasing = TRUE)
#v <- sort(rowSums(ipcc_ftm),decreasing=TRUE)
#d_total <- data.frame(word = names(v),freq=v)
d <- data.frame(ipcc_ftm) #export this dataframe for CHO!
d$total <- apply(d, 1, sum)
View(d)
corp <- VCorpus(URISource(files),
readerControl = list(reader = readPDF))
#remove stop words
ipcc_corpus <- tm_map(corp, removeWords, stopwords("english"))
#remove punctuation
ipcc_corpus <- tm_map(corp, content_transformer(removePunctuation))
#remove numbers
#ipcc_corpus <- tm_map(corp, content_transformer(removeNumbers))
#to lowercase
ipcc_corpus <- tm_map(corp,  content_transformer(tolower))
#remove whitespace
ipcc_corpus <- tm_map(corp, content_transformer(stripWhitespace))
#remove special characters and htmp (if present)
removeNonASCII <- function(x) iconv(x, "latin1", "ASCII", sub="")
#removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
#ipcc_corpus <- tm_map(corp, content_transformer(removeURL)) #remove web url
ipcc_corpus <- tm_map(corp, content_transformer(removeNonASCII)) #remove non-ASCII
#lemmatize
ipcc_corpus  <- tm_map(corp, content_transformer(textstem::lemmatize_strings))
ipcc_tdm <- TermDocumentMatrix(ipcc_corpus,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
#stemming = TRUE,
#  removeNumbers = TRUE,
bounds = list(global = c(5, Inf))))
#filter for words that have at least 100 frequency
ipcc_ft <- findFreqTerms(ipcc_tdm, lowfreq = 100, highfreq = Inf)
ipcc_ftm <- as.matrix(ipcc_tdm[ipcc_ft,], id = nDocs(ipcc_tdm))
#v <- sort(apply(ipcc_ftm, 1, sum), decreasing = TRUE)
#v <- sort(rowSums(ipcc_ftm),decreasing=TRUE)
#d_total <- data.frame(word = names(v),freq=v)
d <- data.frame(ipcc_ftm) #export this dataframe for CHO!
d$total <- apply(d, 1, sum)
View(d)
#index to columns
d <- cbind(word = rownames(d), d)
rownames(d) <- 1:nrow(d)
View(d)
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$total, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(lemma = reorder(lemma, n_success)) %>%
#ggplot(aes(lemma, n_success)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in the EU Green Deal") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(lemma = reorder(lemma, n_success)) %>%
ggplot(aes(word, total)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in the EU Green Deal") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
mutate(total = reorder(word, total)) %>%
ggplot(aes(word, total)) +
geom_col() +
labs(x = "Word Frequency", y = NULL) +
ggtitle("Word Frequency in the EU Green Deal") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
mutate(total = reorder(word, total)) %>%
ggplot(aes(word, total)) +
geom_col() +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
mutate(total = reorder(word, total)) %>%
ggplot(aes(total, word)) +
geom_col() +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(word, total)) +
geom_col() +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, -total), total)) +
geom_col() +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col() +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "green") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip()
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip() +
theme(plot. title = element_text(face = "bold"))
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports (1990 - 2014") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
d %>%
arrange(desc(total)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, total), total)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in All IPCC Reports (1990 - 2014)") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
wordcloud(words = d$word, freq = d$total, min.freq = 1,
max.words=800, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$total, min.freq = 1,
max.words=80, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$total, min.freq = 1,
max.words=70, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d %>%
arrange(desc(ipcc_90_92.pdf)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, ipcc_90_92.pdf), ipcc_90_92.pdf)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in the 1990 IPCC Report") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
plot_90 <- d %>%
arrange(desc(ipcc_90_92.pdf)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, ipcc_90_92.pdf), ipcc_90_92.pdf)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in the 1990 IPCC Report") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
plot_95 <- d %>%
arrange(desc(ipcc_95.pdf)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, ipcc_95.pdf), ipcc_95.pdf)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in the 1995 IPCC Report") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
plot_95
plot_01 <- d %>%
arrange(desc(ipcc_01.pdf)) %>%
dplyr::slice(1:10) %>%
#mutate(total = reorder(word, total)) %>%
ggplot(aes(x = reorder(word, ipcc_01.pdf), ipcc_01.pdf)) +
geom_col(fill = "darkgreen") +
labs(x = NULL, y = "Word Frequency") +
ggtitle("Word Frequency in the 1995 IPCC Report") +
coord_flip() +
theme(plot.title = element_text(face = "bold"))
ggplot() +
facet_wrap(~L1,nrow = 2,scales = "free_y") +
geom_bar(data = d,
aes(x = word , y = dipcc_90_92.pdf),
stat = "bin" ) +
#position = "fill") +
geom_bar(data = d,
aes(x = word , y = dipcc_95.pdf),
stat = "bin" ) +
ggtitle("Word Frequency in All IPCC Reports") +
labs(x = "Word", y = "Frequency")
